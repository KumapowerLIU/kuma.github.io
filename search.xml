<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>pix to pix</title>
      <link href="/2018/09/07/first/"/>
      <url>/2018/09/07/first/</url>
      
        <content type="html"><![CDATA[<p>今天我们来介绍一下pix to pix这篇文章, 这篇文章是2017年备受瞩目的文章之一，其推广的项目以及优秀的实验结果让人折服。<br><img src="https://pic4.zhimg.com/80/v2-318f7faf37807f58bdc54aff7a01fb3b_hd.jpg" alt="实验效果图"><br>pix to pix 也就是图像到图像的生成网络，以往的GAN都是通过随机噪音产生图像，不能精确到人们的所需，于是图像到图像的翻译工作就在这篇文章中实现了。如图上图中左右图的对称，条件对抗网络需要数据集有特定的配对，从而更方便的训练，这也是条件二字的由来。</p><h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><p><img src="https://pic1.zhimg.com/v2-0a9ce9ba4094553a0a14c4d0ddc886b4_r.jpg" alt="生成器架构图"><br>文章通过输入X，即边缘图像，接着通过生成器G，产生G(X)与X对比，此时应该判断G(X)是虚假的图片，同时如果输入X和真实的图片，D应该判断出是真实图片。</p><h4 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h4><p><img src="https://pic2.zhimg.com/v2-25b4b112b58d60c1089b9f5b4418550d_r.jpg" alt="生成器架构图"><br>上图是生成器的模型构建，运用卷积与反卷积构成，由于输入与输出的图像在边缘有相似点。所以把编码器中每一层都接到解码器相对的层上。保障输入输出的衔接，可以用残差网络实现。作者称这个生成器为U-net(结构和U-net很像)。</p><h4 id="补丁辨别器"><a href="#补丁辨别器" class="headerlink" title="补丁辨别器"></a>补丁辨别器</h4><p><img src="https://pic2.zhimg.com/v2-1a2d6e4178cdbe9271ba4e0befdfaa9d_r.jpg" alt="辨别器"><br>在损失函数中，L1被添加进来来保证输入和输出的共性。这就启发出了一个观点，那就是图像的变形分为两种，局部的和全局的。既然L1可以防止全局的变形。那么只要让D去保证局部能够精准即可。</p><p>于是，Pix2Pix中的D被实现为Patch-D，所谓Patch，是指无论生成的图像有多大，将其切分为多个固定大小的Patch输入进D去判断。<br><strong>这样有很多好处：</strong></p><ol><li>D的输入变小，计算量小，训练速度快。</li><li>因为G本身是全卷积的，对图像尺度没有限制。而D如果是按照Patch去处理图像，也对图像大小没有限制。就会让整个Pix2Pix框架对图像大小没有限制。增大了框架的扩展性。</li></ol><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><img src="https://pic2.zhimg.com/80/v2-e46fe08d962a87f2a0db5138dab37381_hd.jpg" alt="Conditional GAN"><br>Conditional GAN与以往的生成模型损失函数有所不同，在辨别器D中加入了输入X从而控制D的精确度，更加切合数据集，同时z是原GAN的噪声部分。<br><img src="https://pic3.zhimg.com/v2-2c6f0d567689d86e1de8d08769f457f2_r.jpg" alt="Fully function"><br>同时还需要L1 loss来控制生成图像与真实图像的差距，这是为了让分辨器D能够更好的关注图像细节以及局部高频信息，而用L1 loss来控制整个图像总体精度，公式4就是文章中最终需要优化的损失函数。其中超参数取10.</p><h3 id="论文地址"><a href="#论文地址" class="headerlink" title="论文地址"></a>论文地址</h3><p>这是去往 <a href="https://arxiv.org/abs/1611.07004" target="_blank" rel="noopener">论文</a> 的链接。</p><h3 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h3><p>这是去往 <a href="https://github.com/KumapowerLIU/pix-to-pix-gan-pytorch" target="_blank" rel="noopener">代码</a> 的链接。</p>]]></content>
      
      
      <categories>
          
          <category> GAN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> GAN </tag>
            
            <tag> pix to pix </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
